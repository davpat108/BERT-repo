{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "20201101.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0tjkF5eGdM9h",
        "outputId": "08a4ec5b-a09c-4a70-c0cc-8584860a48ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers\n",
        "!{sys.executable} -m pip install torch\n",
        "!{sys.executable} -m pip install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSLRGJU4dM9z"
      },
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torchvision import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def preparedata(sentences, tokennumbers):\n",
        "    \n",
        "    RetVal=[]\n",
        "    inputs=tokenizer(sentences, padding = True, truncation = True, return_tensors = \"pt\")\n",
        "    outputs=aModel(**inputs)\n",
        "    for i, tokens in enumerate(outputs[0]):\n",
        "      RetVal.append(tokens[int(tokennumbers[i])])\n",
        "      \n",
        "    return RetVal\n",
        "\n",
        "def casetonumber(case):\n",
        "    switch = {\n",
        "        \"Nom\": 0,\n",
        "        \"Acc\": 1,\n",
        "        \"Ins\": 2,\n",
        "        \"Ine\": 3, \n",
        "        \"Sup\": 4,\n",
        "        \"Sub\": 5\n",
        "    }\n",
        "    \n",
        "    return switch.get(case, \"Invalid case\")\n",
        "\n",
        "\n",
        "def gettokennumber(sentence, wordnumber):\n",
        "\n",
        "    wordlist=sentence.split(\" \")\n",
        "    tokennumber = 0\n",
        "\n",
        "    for i, word in enumerate(wordlist, 0):\n",
        "        output=tokenizer(word, add_special_tokens=False)\n",
        "        tokennumber = tokennumber + len(output[\"input_ids\"])\n",
        "        if i == int(wordnumber):\n",
        "          break\n",
        "    return tokennumber\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-J2stT3dM-C"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "aModel = AutoModel.from_pretrained(\"bert-base-multilingual-cased\", return_dict=True)\n",
        "\n",
        "train_tsv=pd.read_csv('train.tsv',na_filter=None, quoting=3, sep=\"\\t\")\n",
        "dev_tsv=pd.read_csv('dev.tsv',na_filter=None, quoting=3,sep=\"\\t\")\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WctMDp6X3f5X"
      },
      "source": [
        "training_data=[]\n",
        "test_data=[]\n",
        "\n",
        "for i, obj in enumerate(train_tsv.values, 0):\n",
        "      input=tokenizer(obj[0], return_tensors = \"pt\")\n",
        "      output=aModel(**input)\n",
        "      training_data.append([])\n",
        "      training_data[i].append(output.last_hidden_state[0][gettokennumber(obj[0], obj[2])])\n",
        "      training_data[i].append(casetonumber(obj[3]))\n",
        "\n",
        "\n",
        "for i, obj in enumerate(dev_tsv.values):\n",
        "      input=tokenizer(obj[0], padding = True, truncation = True, return_tensors = \"pt\")\n",
        "      output=aModel(**input)\n",
        "\n",
        "      test_data.append([])\n",
        "      test_data[i].append(output.last_hidden_state[0][gettokennumber(obj[0], obj[2])])\n",
        "      test_data[i].append(casetonumber(obj[3]))\n",
        "\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(training_data, batch_size=16, shuffle=True)\n",
        "testloader=torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=True)\n",
        "file1.close()\n",
        "file2.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTdZwHdEdM-V"
      },
      "source": [
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        h = self.input_layer(X)\n",
        "        h = self.relu(h)\n",
        "        out = self.output_layer(h)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPumJn_tdM-f"
      },
      "source": [
        "Net = SimpleClassifier(\n",
        "    input_dim = 768,\n",
        "    output_dim = 6,\n",
        "    hidden_dim = 50\n",
        ")\n",
        "Net\n",
        "Net=Net.cuda()\n",
        "NumEpoch=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AbxQdIUdM-q"
      },
      "source": [
        "\n",
        "\n",
        "def createLoss():\n",
        "    return nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def createOptimizer():\n",
        "    return torch.optim.SGD(\n",
        "        Net.parameters(), lr = 1e-1,\n",
        "        momentum = 0.9, nesterov = True, \n",
        "        weight_decay = 1e-4)\n",
        "\n",
        "\n",
        "\n",
        "def creatScheduler():\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, numEpoch, eta_min=1e-2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuRkHLwidM-y"
      },
      "source": [
        "def train(epoch):\n",
        "    Net.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        tensors, labels = data\n",
        "        \n",
        "        tensors=tensors.cuda()\n",
        "        labels=labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs=Net(tensors)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            running_loss +=loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            \n",
        "    tr_loss = running_loss/i\n",
        "    tr_corr = correct/total*100\n",
        "    print(\"Train epoch \" + str(epoch+1) + \"  correct: \" + str(tr_corr))\n",
        "    return tr_loss, tr_corr\n",
        "\n",
        "def val(epoch):\n",
        "    Net.eval()\n",
        "    running_loss=0.0\n",
        "    correct = 0.0\n",
        "    total=0\n",
        "\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "        tensors, labels = data\n",
        "        tensors=labels.cuda()\n",
        "        inputs=inputs.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs=Net(inputs)\n",
        "            loss = criterion(outputs, tensors)\n",
        "            running_loss +=loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            \n",
        "    val_loss = running_loss/i\n",
        "    val_corr = correct/total * 100\n",
        "    print(\"Test epoch \" + str(epoch + 1) + \" loss: \" + str(running_loss / i) + \" correct: \" +  str(val_corr))\n",
        "    return val_loss, val_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpaAPSwQdM-4"
      },
      "source": [
        "\n",
        "train_accs = []\n",
        "train_losses = []\n",
        "val_accs = []\n",
        "val_losses = []\n",
        "best_acc = 0\n",
        "\n",
        "torch.manual_seed(32)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark=False\n",
        "\n",
        "criterion = createLoss()\n",
        "optimizer = createOptimizer()\n",
        "scheduler = creatScheduler()\n",
        "\n",
        "for epoch in range(numEpoch):\n",
        "    #Train\n",
        "    loss, acc = train(epoch)\n",
        "    train_accs.append(acc)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    #val\n",
        "    loss, acc = val(epoch)\n",
        "    val_accs.append(acc)\n",
        "    val_losses.append(loss)\n",
        "    scheduler.step()\n",
        "    if acc>best_acc:\n",
        "        best_acc = acc\n",
        "        print(\"Best model so far\")\n",
        "        torch.save(Net, \"model.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}