{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "20201104.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0tjkF5eGdM9h"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers\n",
        "!{sys.executable} -m pip install torch\n",
        "!{sys.executable} -m pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSLRGJU4dM9z"
      },
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torchvision import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def preparedata(sentences, tokennumbers):\n",
        "    \n",
        "    RetVal=[]\n",
        "    inputs=tokenizer(sentences, padding = True, truncation = True, return_tensors = \"pt\")\n",
        "    outputs=aModel(**inputs)\n",
        "    for i, tokens in enumerate(outputs[0]):\n",
        "      RetVal.append(tokens[int(tokennumbers[i])])\n",
        "      \n",
        "    return RetVal\n",
        "\n",
        "def casetonumber(case):\n",
        "    switch = {\n",
        "        \"Nom\": 0,\n",
        "        \"Acc\": 1,\n",
        "        \"Ins\": 2,\n",
        "        \"Ine\": 3, \n",
        "        \"Sup\": 4,\n",
        "        \"Sub\": 5\n",
        "    }\n",
        "    return switch.get(case, \"Invalid case\")\n",
        "\n",
        "\n",
        "def gettokennumber(sentence, wordnumber):\n",
        "\n",
        "    wordlist=sentence.split(\" \")\n",
        "    tokennumber = 0\n",
        "\n",
        "    for i, word in enumerate(wordlist, 0):\n",
        "        output=tokenizer(word, add_special_tokens=False)\n",
        "        tokennumber = tokennumber + len(output[\"input_ids\"])\n",
        "        if i == int(wordnumber):\n",
        "          break\n",
        "    return tokennumber\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-J2stT3dM-C"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "aModel = AutoModel.from_pretrained(\"bert-base-multilingual-cased\", return_dict=True)\n",
        "aModel.eval()\n",
        "\n",
        "\n",
        "train_tsv=pd.read_csv('train.tsv',na_filter=None, quoting=3, sep=\"\\t\")\n",
        "dev_tsv=pd.read_csv('dev.tsv',na_filter=None, quoting=3,sep=\"\\t\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WctMDp6X3f5X"
      },
      "source": [
        "training_data=[]\n",
        "test_data=[]\n",
        "\n",
        "\n",
        "\n",
        "for i, obj in enumerate(train_tsv.values, 0):\n",
        "      input=tokenizer(obj[0], padding = True,\n",
        "                      truncation = True, return_tensors = \"pt\")\n",
        "      output=aModel(**input)\n",
        "      training_data.append([])\n",
        "\n",
        "      training_data[i].append(output.last_hidden_state[0][gettokennumber(obj[0], obj[2])].detach())\n",
        "      training_data[i].append(casetonumber(obj[3]))\n",
        "\n",
        "\n",
        "for i, obj in enumerate(dev_tsv.values):\n",
        "      input=tokenizer(obj[0], padding = True,\n",
        "                      truncation = True, return_tensors = \"pt\")\n",
        "      output=aModel(**input)\n",
        "      test_data.append([])\n",
        "      test_data[i].append(output.last_hidden_state[0][gettokennumber(obj[0], obj[2])].detach())\n",
        "      test_data[i].append(casetonumber(obj[3]))\n",
        "\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(training_data, batch_size=16, shuffle=True)\n",
        "testloader=torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTdZwHdEdM-V"
      },
      "source": [
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        h = self.input_layer(X)\n",
        "        h = self.relu(h)\n",
        "        out = self.output_layer(h)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPumJn_tdM-f"
      },
      "source": [
        "Net = SimpleClassifier(\n",
        "    input_dim = 768,\n",
        "    output_dim = 6,\n",
        "    hidden_dim = 50\n",
        ")\n",
        "Net\n",
        "Net=Net.cuda()\n",
        "numEpoch=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AbxQdIUdM-q"
      },
      "source": [
        "\n",
        "\n",
        "def createLoss():\n",
        "    return nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def createOptimizer():\n",
        "    return torch.optim.SGD(\n",
        "        Net.parameters(), lr = 1e-1,\n",
        "        momentum = 0.9, nesterov = True, \n",
        "        weight_decay = 1e-4)\n",
        "\n",
        "\n",
        "\n",
        "def creatScheduler():\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, numEpoch, eta_min=1e-2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuRkHLwidM-y"
      },
      "source": [
        "def train(epoch):\n",
        "    Net.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        tensors, labels = data\n",
        "\n",
        "        tensors=tensors.cuda()\n",
        "        labels=labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs=Net(tensors)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            running_loss +=loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            \n",
        "    tr_loss = running_loss/i\n",
        "    tr_corr = correct/total*100\n",
        "    print(\"Train epoch \" + str(epoch+1) + \"  correct: \" + str(tr_corr))\n",
        "    return tr_loss, tr_corr\n",
        "\n",
        "def val(epoch):\n",
        "    Net.eval()\n",
        "    running_loss=0.0\n",
        "    correct = 0.0\n",
        "    total=0\n",
        "\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "        tensors, labels = data\n",
        "\n",
        "        tensors=tensors.cuda()\n",
        "        labels=labels.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs=Net(tensors)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss +=loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            \n",
        "    val_loss = running_loss/i\n",
        "    val_corr = correct/total * 100\n",
        "    print(\"Test epoch \" + str(epoch + 1) + \" loss: \" + str(running_loss / i) + \" correct: \" +  str(val_corr))\n",
        "    return val_loss, val_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpaAPSwQdM-4",
        "outputId": "ff5a99fd-e56a-4570-bc53-c0d4d091f61a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "train_accs = []\n",
        "train_losses = []\n",
        "val_accs = []\n",
        "val_losses = []\n",
        "best_acc = 0\n",
        "\n",
        "torch.manual_seed(32)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark=False\n",
        "\n",
        "criterion = createLoss()\n",
        "optimizer = createOptimizer()\n",
        "scheduler = creatScheduler()\n",
        "\n",
        "for epoch in range(numEpoch):\n",
        "    #Train\n",
        "    loss, acc = train(epoch)\n",
        "    train_accs.append(acc)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    #val\n",
        "    loss, acc = val(epoch)\n",
        "    val_accs.append(acc)\n",
        "    val_losses.append(loss)\n",
        "    scheduler.step()\n",
        "    if acc>best_acc:\n",
        "        best_acc = acc\n",
        "        print(\"Best model so far\")\n",
        "        torch.save(Net, \"model.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train epoch 1  correct: 97.37991266375546\n",
            "Test epoch 1 loss: 0.15884615509033514 correct: 95.07389162561576\n",
            "Best model so far\n",
            "Train epoch 2  correct: 97.99126637554585\n",
            "Test epoch 2 loss: 0.20432720853326222 correct: 96.05911330049261\n",
            "Best model so far\n",
            "Train epoch 3  correct: 98.8646288209607\n",
            "Test epoch 3 loss: 0.21435737803888819 correct: 94.08866995073892\n",
            "Train epoch 4  correct: 97.90393013100437\n",
            "Test epoch 4 loss: 0.21034145770439258 correct: 96.05911330049261\n",
            "Train epoch 5  correct: 98.95196506550218\n",
            "Test epoch 5 loss: 0.10043548242234597 correct: 97.53694581280789\n",
            "Best model so far\n",
            "Train epoch 6  correct: 99.56331877729258\n",
            "Test epoch 6 loss: 0.1319894484844847 correct: 97.53694581280789\n",
            "Train epoch 7  correct: 99.65065502183405\n",
            "Test epoch 7 loss: 0.19107528881067992 correct: 96.55172413793103\n",
            "Train epoch 8  correct: 99.91266375545852\n",
            "Test epoch 8 loss: 0.18619686130596827 correct: 97.53694581280789\n",
            "Train epoch 9  correct: 99.91266375545852\n",
            "Test epoch 9 loss: 0.1726580675187582 correct: 97.04433497536947\n",
            "Train epoch 10  correct: 99.91266375545852\n",
            "Test epoch 10 loss: 0.168685845901526 correct: 97.04433497536947\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}